import os
from dotenv import load_dotenv
load_dotenv()
import streamlit as st
import tempfile
from main import DocumentLoader, EmbeddingModel, VectorStore
from langchain.chat_models import ChatOpenAI
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import PromptTemplate
from langchain.schema.runnable import RunnablePassthrough

from main import VectorStore
from main import DocumentLoader
from main import EmbeddingModel
from main import llm_model
from main import load_config

st.set_page_config(page_title="PDF ì±—ë´‡", layout="wide")

st.title("ğŸ“š PDF ì±—ë´‡")
st.write("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

config = load_config("config.yaml")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'vector_store' not in st.session_state:
    st.session_state.vector_store = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

# PDF íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])

if uploaded_file is not None:
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    # PDF ì²˜ë¦¬
    with st.spinner('PDFë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
        try:
            loader = DocumentLoader(tmp_file_path)
            hf_embeddings = EmbeddingModel()
            st.session_state.vector_store = VectorStore(loader.splits, hf_embeddings.embed_model())
            st.success('PDF ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!')
        except Exception as e:
            st.error(f'PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}')
    
    # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    os.unlink(tmp_file_path)

# ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
if st.session_state.vector_store is not None:
    # RAG ì²´ì¸ ì„¤ì •
    template = config["custom_template"]
    rag_prompt_custom = PromptTemplate.from_template(template=template)
    
    llm_model = llm_model()
    retriever = st.session_state.vector_store.database.as_retriever(embeddings=OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY")))
    rag_chain = {"context": retriever, "question": RunnablePassthrough()} | rag_prompt_custom | llm_model

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
    for message in st.session_state.chat_history:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_history.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.write(prompt)

        # ì±—ë´‡ ì‘ë‹µ
        with st.chat_message("assistant"):
            with st.spinner('ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
                response = rag_chain.invoke(prompt)
                st.write(response.content)
                st.session_state.chat_history.append({"role": "assistant", "content": response.content})
else:
    st.info('PDF íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.')
